<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Tu Vu</title> <meta name="author" content="Tu Vu"> <meta name="description" content="For an up-to-date list of my research papers, please see my &lt;a href='https://scholar.google.com/citations?user=tOevwEEAAAAJ&amp;hl=en'&gt;Google Scholar&lt;/a&gt; profile. * denotes equal contribution."> <meta name="keywords" content="llms, nlp, ai"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link href="https://fonts.googleapis.com/css?family=Roboto%20Slab" rel="stylesheet"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.jpg?2ee9156889900d8999c460d375b97cbe"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tuvllms.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tu </span><span class="font-weight-bold">Vu</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/cv.pdf">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="color: var(--global-title-color);">publications</h1> <p class="post-description">For an up-to-date list of my research papers, please see my <a href="https://scholar.google.com/citations?user=tOevwEEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Google Scholar</a> profile. * denotes equal contribution.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="vu-etal-2024-foundational" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation</div> <div class="author"> <strong>Tu Vu*</strong>, <span style="color: var(--global-coauthor-color);">Kalpesh Krishna*</span>, <span style="color: var(--global-coauthor-color);">Salaheddin Alzubi</span>, <span style="color: var(--global-coauthor-color);">Chris Tar</span>, <span style="color: var(--global-coauthor-color);">Manaal Faruqui</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Yun-Hsuan Sung</span> </div> <div class="periodical"> <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2407.10817" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2407.10817.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on our large and diverse collection of 100+ quality assessment tasks comprising 5M+ human judgments, curated and standardized using publicly released human evaluations from previous research. FLAMe significantly improves generalization to a wide variety of held-out tasks, outperforming LLMs trained on proprietary data like GPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a powerful starting point for further downstream fine-tuning, using reward modeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our FLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative model trained exclusively on permissively licensed data, outperforming both GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more computationally efficient approach using a novel tail-patch fine-tuning strategy to optimize our FLAMe multitask mixture for reward modeling evaluation (FLAMe-Opt-RM), offering competitive RewardBench performance while requiring approximately 25x less training datapoints. Overall, our FLAMe variants outperform all popular proprietary LLM-as-a-Judge models we consider across 8 out of 12 autorater evaluation benchmarks, encompassing 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals that FLAMe is significantly less biased than these LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark, while effectively identifying high-quality responses for code generation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vu-etal-2024-foundational</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vu*, Tu and Krishna*, Kalpesh and Alzubi, Salaheddin and Tar, Chris and Faruqui, Manaal and Sung, Yun-Hsuan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2407.10817}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2407.10817.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <span class="highlight-text" style="color: var(--global-highlight-color);"><i>// The top-performing generative model on <a href="https://huggingface.co/spaces/allenai/reward-bench" style="color: #d92f7c;" rel="external nofollow noopener" target="_blank">RewardBench</a> trained solely on publicly available data</i></span> </div> </div> </li> <li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">Preprint</abbr></div> <div id="yadav-etal-2024-matters" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">What Matters for Model Merging at Scale?</div> <div class="author"> <span style="color: var(--global-coauthor-color);">Prateek Yadav</span>, <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">Jonathan Lai</span>, <span style="color: var(--global-coauthor-color);">Alexandra Chronopoulou</span>, <span style="color: var(--global-coauthor-color);">Manaal Faruqui</span>, <span style="color: var(--global-coauthor-color);">Mohit Bansal</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Tsendsuren Munkhdalai</span> </div> <div class="periodical"> <em>In arXiv preprint arXiv:2410.03617</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.03617" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2410.03617" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Model merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors – like the base model quality and number of expert models – , to affect the merged model’s performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. We experiment with merging fully fine-tuned models using 4 popular merging methods – Averaging, Task Arithmetic, Dare, and TIES – across model sizes ranging from 1B-64B parameters and merging up to 8 different expert models. We evaluate the merged models on both held-in tasks, i.e., the expert’s training tasks, and zero-shot generalization to unseen held-out tasks. Our experiments provide several new insights about model merging at scale and the interplay between different factors. First, we find that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging 8 large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, we can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, our findings shed light on some interesting properties of model merging while also highlighting some limitations. We hope that this study will serve as a reference point on large-scale merging for upcoming research.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yadav-etal-2024-matters</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{What Matters for Model Merging at Scale?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yadav, Prateek and Vu, Tu and Lai, Jonathan and Chronopoulou, Alexandra and Faruqui, Manaal and Bansal, Mohit and Munkhdalai, Tsendsuren}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2410.03617}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2410.03617}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div> <div id="vu-etal-2024-freshllms" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">FreshLLMs: Refreshing large language models with search engine augmentation</div> <div class="author"> <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">Mohit Iyyer</span>, <span style="color: var(--global-coauthor-color);">Xuezhi Wang</span>, <span style="color: var(--global-coauthor-color);">Noah Constant</span>, <span style="color: var(--global-coauthor-color);">Jerry Wei</span>, <span style="color: var(--global-coauthor-color);">Jason Wei</span>, <span style="color: var(--global-coauthor-color);">Chris Tar</span>, <span style="color: var(--global-coauthor-color);">Yun-Hsuan Sung</span>, <span style="color: var(--global-coauthor-color);">Denny Zhou</span>, <span style="color: var(--global-coauthor-color);">Quoc Le</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Thang Luong</span> </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: ACL 2024</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.03214" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2310.03214.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vu-etal-2024-freshllms</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{F}resh{LLM}s: Refreshing large language models with search engine augmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and Tar, Chris and Sung, Yun-Hsuan and Zhou, Denny and Le, Quoc and Luong, Thang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of the Association for Computational Linguistics: ACL 2024}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2310.03214.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <span class="highlight-text" style="color: var(--global-highlight-color);"><i>// Our dataset and method have inspired or been used for the development of <a href="https://gemini.google.com" style="color: #d92f7c;" rel="external nofollow noopener" target="_blank">Google’s Gemini</a>, <a href="https://blog.perplexity.ai/blog/introducing-pplx-online-llms" style="color: #d92f7c;" rel="external nofollow noopener" target="_blank">Perplexity.AI’s Online LLMs</a>, <a href="https://about.you.com/introducing-the-you-api-web-scale-search-for-llms" style="color: #d92f7c;" rel="external nofollow noopener" target="_blank">You.com</a>, and <a href="https://contextual.ai/introducing-rag2/" style="color: #d92f7c;" rel="external nofollow noopener" target="_blank">Contextual AI’s RAG 2.0</a></i></span> </div> </div> </li> <li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="shen2023mixture" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">Mixture-of-experts meets instruction tuning: A winning combination for large language models</div> <div class="author"> <span style="color: var(--global-coauthor-color);">Sheng Shen</span>, <span style="color: var(--global-coauthor-color);">Le Hou</span>, <span style="color: var(--global-coauthor-color);">Yanqi Zhou</span>, <span style="color: var(--global-coauthor-color);">Nan Du</span>, <span style="color: var(--global-coauthor-color);">Shayne Longpre</span>, <span style="color: var(--global-coauthor-color);">Jason Wei</span>, <span style="color: var(--global-coauthor-color);">Hyung Won Chung</span>, <span style="color: var(--global-coauthor-color);">Barret Zoph</span>, <span style="color: var(--global-coauthor-color);">William Fedus</span>, <span style="color: var(--global-coauthor-color);">Xinyun Chen</span>, <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">Yuexin Wu</span>, <span style="color: var(--global-coauthor-color);">Wuyang Chen</span>, <span style="color: var(--global-coauthor-color);">Albert Webson</span>, <span style="color: var(--global-coauthor-color);">Yunxuan Li</span>, <span style="color: var(--global-coauthor-color);">Vincent Zhao</span>, <span style="color: var(--global-coauthor-color);">Hongkun Yu</span>, <span style="color: var(--global-coauthor-color);">Kurt Keutzer</span>, <span style="color: var(--global-coauthor-color);">Trevor Darrell</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Denny Zhou</span> </div> <div class="periodical"> <em>In Proceedings of the 12th International Conference on Learning Representations</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.14705" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2305.14705.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be utilized to add learnable parameters to Large Language Models (LLMs) without increasing inference cost. Instruction tuning is a technique for training LLMs to follow instructions. We advocate combining these two approaches, as we find that MoE models benefit more from instruction tuning than dense models. In particular, we conduct empirical studies across three experimental setups: (i) Direct finetuning on individual downstream tasks devoid of instruction tuning; (ii) Instructiontuning followed by in-context few-shot or zero-shot generalization on downstream tasks; and (iii) Instruction tuning supplemented by further finetuning on individual downstream tasks. In the first scenario, MoE models overall underperform dense models of identical computational capacity. This narrative, however, dramatically changes with the introduction of instruction tuning (second and third scenario), used independently or in conjunction with task-specific finetuning. Our most powerful model, FLAN-MOE-32B, surpasses the performance of FLAN-PALM-62B on four benchmark tasks, while using only a third of the FLOPs. The advancements embodied byFLAN-MOE inspire a reevaluation of the design principles of large-scale, high-performance language models in the framework of task-agnostic learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shen2023mixture</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mixture-of-experts meets instruction tuning: A winning combination for large language models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shen, Sheng and Hou, Le and Zhou, Yanqi and Du, Nan and Longpre, Shayne and Wei, Jason and Chung, Hyung Won and Zoph, Barret and Fedus, William and Chen, Xinyun and Vu, Tu and Wu, Yuexin and Chen, Wuyang and Webson, Albert and Li, Yunxuan and Zhao, Vincent and Yu, Hongkun and Keutzer, Kurt and Darrell, Trevor and Zhou, Denny}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 12th International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2305.14705.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">Preprint</abbr></div> <div id="geminiteam2024gemini" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">Gemini: A Family of Highly Capable Multimodal Models</div> <div class="author"> <span style="color: var(--global-coauthor-color);">Google Gemini Team</span><span>: </span><span style="color: var(--global-coauthor-color);">Rohan Anil</span>, <span style="color: var(--global-coauthor-color);">Sebastian Borgeaud</span>, <span style="color: var(--global-coauthor-color);">Yonghui Wu</span>, <span style="color: var(--global-coauthor-color);">Jean-Baptiste Alayrac</span>, <span style="color: var(--global-coauthor-color);">Jiahui Yu</span>, <span style="color: var(--global-coauthor-color);">Radu Soricut</span>, <span style="color: var(--global-coauthor-color);">Johan Schalkwyk</span>, <span style="color: var(--global-coauthor-color);">Andrew Dai</span>, <span style="color: var(--global-coauthor-color);">Anja Hauth</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);"> others</span> <span style="color: var(--global-coauthor-color);">including</span> <strong>Tu Vu</strong> </div> <div class="periodical"> <em>In arXiv preprint arXiv:2312.11805</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2312.11805" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2312.11805" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">geminiteam2024gemini</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gemini: A Family of Highly Capable Multimodal Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew and Hauth, Anja and others}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2312.11805}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2312.11805}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <span class="highlight-text" style="color: var(--global-highlight-color);"><i>// <a href="https://blog.google/technology/ai/google-gemini-ai" style="color: var(--global-highlight-color);" rel="external nofollow noopener" target="_blank">Google AI Blog</a></i></span> </div> </div> </li> <li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div> <div id="pmlr-v202-longpre23a" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">The Flan Collection: Designing Data and Methods for Effective Instruction Tuning</div> <div class="author"> <span style="color: var(--global-coauthor-color);">Shayne Longpre</span>, <span style="color: var(--global-coauthor-color);">Le Hou</span>, <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">Albert Webson</span>, <span style="color: var(--global-coauthor-color);">Hyung Won Chung</span>, <span style="color: var(--global-coauthor-color);">Yi Tay</span>, <span style="color: var(--global-coauthor-color);">Denny Zhou</span>, <span style="color: var(--global-coauthor-color);">Quoc V Le</span>, <span style="color: var(--global-coauthor-color);">Barret Zoph</span>, <span style="color: var(--global-coauthor-color);">Jason Wei</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Adam Roberts</span> </div> <div class="periodical"> <em>In Proceedings of the 40th International Conference on Machine Learning</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2301.13688" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.mlr.press/v202/longpre23a/longpre23a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/google-research/FLAN/tree/main/flan/v2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> </div> <div class="abstract hidden"> <p>We study the design decision of publicly available instruction tuning methods, by reproducing and breaking down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17% across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, chain-of-thought) actually yields equivalent or stronger (2%) performance in all settings. In further experiments we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks – motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pmlr-v202-longpre23a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Flan Collection: Designing Data and Methods for Effective Instruction Tuning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and Roberts, Adam}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 40th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{22631--22648}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{202}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research(PMLR)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v202/longpre23a.html}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v202/longpre23a/longpre23a.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <span class="highlight-text" style="color: var(--global-highlight-color);"><i>// <a href="https://blog.research.google/2023/02/the-flan-collection-advancing-open.html" style="color: var(--global-highlight-color);" rel="external nofollow noopener" target="_blank">Google Research Blog</a></i></span> </div> </div> </li> <li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="ren-etal-2023-self" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">Self-Evaluation Improves Selective Generation in Large Language Models</div> <div class="author"> <span style="color: var(--global-coauthor-color);">Jie Ren</span>, <span style="color: var(--global-coauthor-color);">Yao Zhao</span>, <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">Peter J Liu</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Balaji Lakshminarayanan</span> </div> <div class="periodical"> <em>In Proceedings on "I Can’t Believe It’s Not Better! - Failure Modes in the Age of Foundation Models" at NeurIPS 2023 Workshops</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2312.09300" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2312.09300.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs’ superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a "None of the above" option to express the model’s uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ren-etal-2023-self</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Self-Evaluation Improves Selective Generation in Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ren, Jie and Zhao, Yao and Vu, Tu and Liu, Peter J and Lakshminarayanan, Balaji}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings on "I Can't Believe It's Not Better!  - Failure Modes in the Age of Foundation Models" at NeurIPS 2023 Workshops}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2312.09300.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div> <div id="sun-etal-2023-dialect" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">Dialect-robust Evaluation of Generated Text</div> <div class="author"> <span style="color: var(--global-coauthor-color);">Jiao Sun</span>, <span style="color: var(--global-coauthor-color);">Thibault Sellam</span>, <span style="color: var(--global-coauthor-color);">Elizabeth Clark</span>, <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">Timothy Dozat</span>, <span style="color: var(--global-coauthor-color);">Dan Garrette</span>, <span style="color: var(--global-coauthor-color);">Aditya Siddhant</span>, <span style="color: var(--global-coauthor-color);">Jacob Eisenstein</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Sebastian Gehrmann</span> </div> <div class="periodical"> <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2211.00922" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2211.00922.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Text generation metrics that are not robust to dialect variation make it impossible to tell how well systems perform for many groups of users, and can even penalize systems for producing text in lower-resource dialects. In this paper, we introduce a suite of methods to assess whether metrics are dialect robust. These methods show that state-of-the-art metrics are not dialect robust: they often prioritize dialect similarity over semantics, preferring outputs that are semantically incorrect over outputs that match the semantics of the reference but contain dialect differences. As a step towards dialect-robust metrics for text generation, we propose NANO, which introduces regional and language information to the metric’s pretraining. NANO significantly improves dialect robustness while preserving the correlation between automated metrics and human ratings. It also enables a more ambitious approach to evaluation, dialect awareness, in which system outputs are scored by both semantic match to the reference and appropriateness in any specified dialect.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sun-etal-2023-dialect</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dialect-robust Evaluation of Generated Text}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sun, Jiao and Sellam, Thibault and Clark, Elizabeth and Vu, Tu and Dozat, Timothy and Garrette, Dan and Siddhant, Aditya and Eisenstein, Jacob and Gehrmann, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.acl-long.331}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6010--6028}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2211.00922.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div> <div id="vu-etal-2022-spot" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer</div> <div class="author"> <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">Brian Lester</span>, <span style="color: var(--global-coauthor-color);">Noah Constant</span>, <span style="color: var(--global-coauthor-color);">Rami Al-Rfou</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Daniel Cer</span> </div> <div class="periodical"> <em>In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2110.07904" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2110.07904.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>There has been growing interest in parameter-efficient methods to apply pre-trained language models to downstream tasks. Building on the Prompt Tuning approach of Lester et al. (2021), which learns task-specific soft prompts to condition a frozen pre-trained model to perform different tasks, we propose a novel prompt-based transfer learning approach called SPoT: Soft Prompt Transfer. SPoT first learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task. We show that SPoT significantly boosts the performance of Prompt Tuning across many tasks. More remarkably, across all model sizes, SPoT matches or outperforms standard Model Tuning (which fine-tunes all model parameters) on the SuperGLUE benchmark, while using up to 27,000\mbox\times fewer task-specific parameters. To understand where SPoT is most effective, we conduct a large-scale study on task transferability with 26 NLP tasks in 160 combinations, and demonstrate that many tasks can benefit each other via prompt transfer. Finally, we propose an efficient retrieval approach that interprets task prompts as task embeddings to identify similar tasks and predict the most transferable source tasks for a novel target task.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vu-etal-2022-spot</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{SP}o{T}: Better Frozen Model Adaptation through Soft Prompt Transfer}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vu, Tu and Lester, Brian and Constant, Noah and Al-Rfou, Rami and Cer, Daniel}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.acl-long.346}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5039--5059}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2110.07904.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <span class="highlight-text" style="color: var(--global-highlight-color);"><i>// Headlines of Google AI’s Natural Language Accelerated Newsletter Q1, 2022</i></span> </div> </div> </li> <li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="vu-etal-2022-overcoming" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation</div> <div class="author"> <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">Aditya Barua</span>, <span style="color: var(--global-coauthor-color);">Brian Lester</span>, <span style="color: var(--global-coauthor-color);">Daniel Cer</span>, <span style="color: var(--global-coauthor-color);">Mohit Iyyer</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Noah Constant</span> </div> <div class="periodical"> <em>In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2205.12647" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2205.12647.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In this paper, we explore the challenging problem of performing a generative task in a target language when labeled data is only available in English, using summarization as a case study. We assume a strict setting with no access to parallel data or machine translation and find that common transfer learning approaches struggle in this setting, as a generative multilingual model fine-tuned purely on English catastrophically forgets how to generate non-English. Given the recent rise of parameter-efficient adaptation techniques, we conduct the first investigation into how one such method, prompt tuning (Lester et al., 2021), can overcome catastrophic forgetting to enable zero-shot cross-lingual generation. Our experiments show that parameter-efficient prompt tuning provides gains over standard fine-tuning when transferring between less-related languages, e.g., from English to Thai. However, a significant gap still remains between these methods and fully-supervised baselines. To improve cross-lingual transfer further, we explore several approaches, including: (1) mixing in unlabeled multilingual data, and (2) explicitly factoring prompts into recombinable language and task components. Our approaches can provide further quality gains, suggesting that robust zero-shot cross-lingual generation is within reach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vu-etal-2022-overcoming</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vu, Tu and Barua, Aditya and Lester, Brian and Cer, Daniel and Iyyer, Mohit and Constant, Noah}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.emnlp-main.630}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9279--9300}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2205.12647.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="mekala-etal-2022-leveraging" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">Leveraging QA Datasets to Improve Generative Data Augmentation</div> <div class="author"> <span style="color: var(--global-coauthor-color);">Dheeraj Mekala</span>, <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">Timo Schick</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Jingbo Shang</span> </div> <div class="periodical"> <em>In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2205.12604" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2205.12604.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The ability of generative language models (GLMs) to generate text has improved considerably in the last few years, enabling their use for generative data augmentation. In this work, we propose CONDA, an approach to further improve GLM’s ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks. We perform extensive experiments on multiple classification datasets and demonstrate substantial improvements in performance for both few- and zero-shot settings. Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mekala-etal-2022-leveraging</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Leveraging {QA} Datasets to Improve Generative Data Augmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mekala, Dheeraj and Vu, Tu and Schick, Timo and Shang, Jingbo}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.emnlp-main.660}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9737--9750}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2205.12604.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="vu-etal-2021-strata" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">STraTA: Self-Training with Task Augmentation for Better Few-shot Learning</div> <div class="author"> <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">Thang Luong</span>, <span style="color: var(--global-coauthor-color);">Quoc Le</span>, <span style="color: var(--global-coauthor-color);">Grady Simon</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Mohit Iyyer</span> </div> <div class="periodical"> <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2109.06270" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2109.06270.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Despite their recent successes in tackling many NLP tasks, large-scale pre-trained language models do not perform as well in few-shot settings where only a handful of training examples are available. To address this shortcoming, we propose STraTA, which stands for Self-Training with Task Augmentation, an approach that builds on two key ideas for effective leverage of unlabeled data. First, STraTA uses task augmentation, a novel technique that synthesizes a large amount of data for auxiliary-task fine-tuning from target-task unlabeled texts. Second, STraTA performs self-training by further fine-tuning the strong base model created by task augmentation on a broad distribution of pseudo-labeled data. Our experiments demonstrate that STraTA can substantially improve sample efficiency across 12 few-shot benchmarks. Remarkably, on the SST-2 sentiment dataset, STraTA, with only 8 training examples per class, achieves comparable results to standard fine-tuning with 67K training examples. Our analyses reveal that task augmentation and self-training are both complementary and independently effective.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vu-etal-2021-strata</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{ST}ra{TA}: Self-Training with Task Augmentation for Better Few-shot Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vu, Tu and Luong, Thang and Le, Quoc and Simon, Grady and Iyyer, Mohit}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.emnlp-main.462}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5715--5731}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2109.06270.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="vu-etal-2020-exploring" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">Exploring and Predicting Transferability across NLP Tasks</div> <div class="author"> <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">Tong Wang</span>, <span style="color: var(--global-coauthor-color);">Tsendsuren Munkhdalai</span>, <span style="color: var(--global-coauthor-color);">Alessandro Sordoni</span>, <span style="color: var(--global-coauthor-color);">Adam Trischler</span>, <span style="color: var(--global-coauthor-color);">Andrew Mattarella-Micke</span>, <span style="color: var(--global-coauthor-color);">Subhransu Maji</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Mohit Iyyer</span> </div> <div class="periodical"> <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2005.00770" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2005.00770.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling). Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even with low-data source tasks that differ substantially from the target task (e.g., part-of-speech tagging transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as data size, task and domain similarity, and task complexity all play a role in determining transferability.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vu-etal-2020-exploring</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring and Predicting Transferability across {NLP} Tasks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vu, Tu and Wang, Tong and Munkhdalai, Tsendsuren and Sordoni, Alessandro and Trischler, Adam and Mattarella-Micke, Andrew and Maji, Subhransu and Iyyer, Mohit}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.emnlp-main.635}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7882--7926}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2005.00770.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div> <div id="vu-iyyer-2019-encouraging" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">Encouraging Paragraph Embeddings to Remember Sentence Identity Improves Classification</div> <div class="author"> <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Mohit Iyyer</span> </div> <div class="periodical"> <em>In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1906.03656" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1906.03656.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>While paragraph embedding models are remarkably effective for downstream classification tasks, what they learn and encode into a single vector remains opaque. In this paper, we investigate a state-of-the-art paragraph embedding method proposed by Zhang et al. (2017) and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not. We formulate a sentence content task to probe for this basic linguistic property and find that even a much simpler bag-of-words method has no trouble solving it. This result motivates us to replace the reconstruction-based objective of Zhang et al. (2017) with our sentence content probe objective in a semi-supervised setting. Despite its simplicity, our objective improves over paragraph reconstruction in terms of (1) downstream classification accuracies on benchmark datasets, (2) faster training, and (3) better generalization ability.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vu-iyyer-2019-encouraging</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Encouraging Paragraph Embeddings to Remember Sentence Identity Improves Classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vu, Tu and Iyyer, Mohit}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/P19-1638}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6331--6338}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/1906.03656.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">NAACL</abbr></div> <div id="vu-etal-2018-sentence" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">Sentence Simplification with Memory-Augmented Neural Networks</div> <div class="author"> <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">Baotian Hu</span>, <span style="color: var(--global-coauthor-color);">Tsendsuren Munkhdalai</span>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Hong Yu</span> </div> <div class="periodical"> <em>In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1804.07445" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1804.07445.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Sentence simplification aims to simplify the content and structure of complex sentences, and thus make them easier to interpret for human readers, and easier to process for downstream NLP applications. Recent advances in neural machine translation have paved the way for novel approaches to the task. In this paper, we adapt an architecture with augmented memory capacities called Neural Semantic Encoders (Munkhdalai and Yu, 2017) for sentence simplification. Our experiments demonstrate the effectiveness of our approach on different simplification datasets, both in terms of automatic evaluation measures and human judgments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vu-etal-2018-sentence</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sentence Simplification with Memory-Augmented Neural Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vu, Tu and Hu, Baotian and Munkhdalai, Tsendsuren and Yu, Hong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/N18-2013}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{79--85}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/1804.07445.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row" style="width: site.max_width;"> <div class="col-sm-2 abbr"><abbr class="badge">*SEM@NAACL</abbr></div> <div id="vu-shwartz-2018-integrating" class="col-sm-8"> <div class="title" style="font-weight-light; color: var(--global-theme-color);">Integrating Multiplicative Features into Supervised Distributional Methods for Lexical Entailment</div> <div class="author"> <strong>Tu Vu</strong>, <span style="color: var(--global-coauthor-color);">and </span><span style="color: var(--global-coauthor-color);">Vered Shwartz</span> </div> <div class="periodical"> <em>In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1804.08845" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1804.08845.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Supervised distributional methods are applied successfully in lexical entailment, but recent work questioned whether these methods actually learn a relation between two words. Specifically, Levy et al. (2015) claimed that linear classifiers learn only separate properties of each word. We suggest a cheap and easy way to boost the performance of these methods by integrating multiplicative features into commonly used representations. We provide an extensive evaluation with different classifiers and evaluation setups, and suggest a suitable evaluation setup for the task, eliminating biases existing in previous ones.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vu-shwartz-2018-integrating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Integrating Multiplicative Features into Supervised Distributional Methods for Lexical Entailment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vu, Tu and Shwartz, Vered}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/S18-2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{160--166}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/1804.08845.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Tu Vu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: January 04, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?978c66e7a55204094b1af84594915583"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-2BQB5QYNS0"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-2BQB5QYNS0");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>